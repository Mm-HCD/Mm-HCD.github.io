<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png">
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css">
    <meta charset="utf-8">
    <meta name="description" content="Multimodal Frameworks in Healthcare Diagnostics 2025">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Workshop, Reasoning, Learning, Spatio-Temporal Information">
    <meta name="google-site-verification" content="PmOT4QwF1x9TSSKYFg6if9CR_TojuEk8pFDffRgdfns">
    <meta name="twitter:card" content="summary_large_image">
    <meta property="og:title" content="Mm-HCD:Multimodal Frameworks in Healthcare Diagnostics ">
    <meta property="og:description" content="In this workshop, we invite the research community in artificial intelligence to submit works related to the integration of spatial and temporal reasoning with machine learning.">
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('consent', 'default', {
        'ad_storage': 'denied',
        'ad_user_data': 'denied',
        'ad_personalization': 'denied',
        'analytics_storage': 'denied'
      });
    </script> 
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-P77LCK8QPT"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-P77LCK8QPT');
    </script>
    <title>Mm-HCD : Multimodal Frameworks in Healthcare Diagnostics</title>
</head>

<body>

    <div class="banner">
        <img src="assets/Main.jpeg" alt="Workshop Banner" style="width: 100%">
        <div class="top-left">
            <span class="title1"> Special Session  </span> <br>
            <span class="title2"> Mm-HCD:</span> <br>
            <span class="title2"> Multimodal Frameworks</span> <br>
            <span class="title2"> in Healthcare Diagnostics</span> <br>
<!--             <span class="year">MIUA 2025</span> <br> -->
        </div>
        <div class="bottom-right">
             <a target="_blank" href="https://conferences.leeds.ac.uk/miua/">MIUA 2025</a>
        </div>
    </div>

    <table class="navigation">
        <tr>
            <!--<td class="navigation">
                <a title="Conference Home Page" href="#top">Home</a>
            </td>-->
            <td class="navigation">
                <a title="Workshop Program" href="#Program">Program</a> 
            </td>
            <td class="navigation">
                <a title="Call For Papers" href="#CFP">CFP</a>
            </td>
            <td class="navigation">
                <a title="Key Dates" href="#Dates">Dates</a>
            </td>
            <td class="navigation">
                <a title="Submission" href="#Submission">Submission</a>
            </td>
            <td class="navigation">
                <a title="Organization" href="#Organization">Organization</a>
            </td>
            <td class="navigation">
                <a title="Venue" href="#Venue">Venue</a>
            </td>
        </tr>
    </table>

    <h2>Mm-HCD 2025 @ MIUA 2025, Leeds, United Kingdom</h2>
<!--     <p>You may find details about the third edition of STRL at: <a target="_blank" href="https://www.lirmm.fr/strl2024/">https://www.lirmm.fr/strl2024/</a></p> -->

    <h2>Introduction</h2>
    <p>Multimodal models in machine learning, particularly deep learning, are transforming medical image analysis by incorporating diverse data modalities, such as medical imaging, digital pathology data, lab reports, patient surveys, audio data, health records, and more. By combining data from multiple modalities (e.g., images, texts, and audio), models offer a more holistic understanding of the data. This comprehensive perspective enhances the accuracy and reliability of detection and diagnostic tasks. In addition, these models pave the way for personalised treatment approaches by providing a thorough understanding of patient conditions. In conclusion, multimodal models represent a pivotal step toward achieving a more accurate, robust, and interpreted solution in healthcare and beyond, underscoring their growing importance across research and real-world applications.</p>
    <p><a href="#top">Go back to the top</a></p>
    
    <h2 id="CFP">Call For Papers</h2>
    
    <p>In this special session, we invite submissions that investigate innovative multimodal approaches in healthcare diagnostics. We are looking for original, high-quality papers on the following topics of interest, but not limited to:</p>
    <h3>Topics</h3>
    <ul>
    <li><strong>Integration of Imaging Modalities</strong>: Techniques combining MRI, CT scans, genomic sequencing, etc for enhanced disease detection and diagnosis.</li>
    <li><strong>Clinical Validation</strong>: Studies validating multimodal approaches in real-world clinical settings</li>
    <li><strong>Data Fusion Techniques</strong>:Methods for effective data fusion from heterogeneous sources.</li>
    <li><strong>Machine Learning and AI</strong>: Applications of deep learning and AI in analysing multimodal data.</li>
<li><strong>Wearable Technology</strong>: Patient-Reported Outcomes: Incorporating qualitative data from patients to improve diagnostic processes.</li>
<li><strong>Interdisciplinary Approaches</strong>: Collaborations between fields such as bioinformatics, radiology, and public health.</li>
<li><strong>Case Studies and Applications</strong>: Real-world implementations of multimodal approaches in clinical settings.</li>
    </ul> 

    <h2 id="Dates">Key Dates</h2>
    <ul>
    <li><strong>Paper Submission Deadline: Monday 24 March 2025</li>
    <li><strong>Author Notification : Friday 2 May 2025</li>
    <li><strong>Camera-ready regular papers due: Monday 2 June 2025 </li>
    <li>Early bird and oral presenters registration deadline: Monday 9 June 2025</li>
    <li>Registration close: Friday 20 June 2025</li>
    <li>Conference: Tuesday 15 - Thursday 17 July 2025</li>
    
    </ul> 
<p><em><u>Note</u></em>: The  deadline will be 11:59, Greenwich Mean Time (GMT), on Monday 24 March 2025.</p> 
    
    <h2 id="Submission">Submission</h2>
    <p>Submission is now open via CMT, <a target="_blank" href="https://cmt3.research.microsoft.com/User/Login?ReturnUrl=%2FMIUA2025%2F"> click here to submit</a>.</p>
    <p>For more information on how to create a CMT account, <a target="_blank" href="https://cmt3.research.microsoft.com/docs/help/general/account-creation.html"> click here</a>.</p>
    
    <h4>Guidelines</h4>
    <p>Authors are invited to submit full papers of length between 8 and 15 pages (1 column â€“ the LNCS template, <a target="_blank" href="https://www.ijcai.org/authors_kit">word </a>, <a target="_blank" href=" https://resource-cms.springernature.com/springer-cms/rest/v1/content/19238648/data/v8">latex </a>,<a target="_blank" href=" https://www.overleaf.com/latex/templates/springer-lecture-notes-in-computer-science/kzwwpvhwnvfj#.WuA4JS5uZpi"> overleaf </a>) and following the <a target="_blank" href="https://www.ijcai.org/authors_kit">Springer Author Guidelines </a>, showing original research contributions related to the Special session theme. All submissions will be double-blind peer-reviewed and accepted articles will be published as MIUA Proceedings by the Springer Publishing Group.</p>

<!--     <p>Papers should be formatted according to the <a target="_blank" href="https://www.ijcai.org/authors_kit">IJCAI-ECAI 2022 formatting guidelines</a> and submitted as a single PDF file. We welcome submissions across the full spectrum of theoretical and practical work including research ideas, methods, tools, simulations, applications or demos, practical evaluations, and surveys. Submissions that are <em>2 pages long</em> (excluding references and appendices) will be considered for a <em>poster</em>, and submissions that are <em>at least 4 pages and up to 6 pages long</em> (excluding references and appendices) will be considered for an <em>oral presentation</em>. All papers will be peer-reviewed in a <em>single-blind</em> process and assessed based on their novelty, technical quality, potential impact, clarity, and reproducibility (when applicable). Workshop submissions will be handled by EasyChair; <strong>the submission link is as follows</strong>: <a href="https://easychair.org/conferences/?conf=strl2022" target="_blank">https://easychair.org/conferences/?conf=strl2022</a></p> -->

    <p>All questions about submissions should be emailed to mhcd.miua@gmail.com</p>

    

    <h3>Proceedings</h3>

    <p>The accepted papers will appear on the workshop website. We also intend to publish the workshop proceedings with <a target="_blank" href="http://ceur-ws.org/">CEUR-WS.org</a>; this option will be discussed with the authors of accepted papers and is subject to the CEUR-WS.org preconditions. We note that, as STRL 2022 is a workshop, not a conference, submission of the same paper to conferences or journals is acceptable from our standpoint.</p>
    <p><strong>Update</strong>: The back-link to the URL of the workshop proceedings published with CEUR-WS.org is now available at <a target="_blank" href="http://ceur-ws.org/Vol-3190/">http://ceur-ws.org/Vol-3190/</a>.
    <p><a href="#top">Go back to the top</a></p>

    <h2 id="Program">Workshop Program</h2>

<!--     <table>
        <tr>
            <td class="date" rowspan="2">
                9:00
            </td>
            <td class="title-special">
                Welcome and Coffee &amp; Tea!
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table class="plenary">
        <tr>
            <td class="date" rowspan="2">
                9:15
            </td>
            <td class="title">
                Visuospatial Commonsense: On Neurosymbolic Reasoning and Learning about Space and Motion | <a href="files/keynote1.pdf" target="_blank">pdf</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Mehul Bhatt (&Ouml;rebro University, Sweden)
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                10:30
            </td>
            <td class="title">
                Knowing Earlier What Right Means to You: A Comprehensive VQA Dataset for Grounding Relative Directions via Multi-task Learning | <a href="files/paper4.pdf" target="_blank">pdf</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Kyra Ahrens (University of Hamburg, Germany)<br>Matthias Kerzel (University of Hamburg, Germany)<br>Jae Hee Lee (University of Hamburg, Germany)<br>Cornelius Weber (University of Hamburg, Germany)<br>Stefan Wermter (University of Hamburg, Germany)
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                11:00
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                11:15
            </td>
            <td class="title">
                Uncertainty-aware Evaluation of Time-series Classification for Online Handwriting Recognition with Domain Shift | <a href="files/paper3.pdf" target="_blank">pdf</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Andreas Kla&szlig; (Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany + LMU Munich, Germany)<br>Sven M. Lorenz (Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany + LMU Munich, Germany)<br>Martin W. Lauer-Schmaltz (Technical University of Denmark, Denmark)<br>David R&uuml;gamer (LMU Munich, Germany + RWTH Aachen, Germany)<br>Bernd Bischl (LMU Munich, Germany)<br>Christopher Mutschler (Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany)<br>Felix Ott (Fraunhofer IIS, Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany + LMU Munich, Germany)
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                11:45
            </td>
            <td class="title">
                Spatial-temporal Transformer Network with Self-supervised Learning for Traffic Flow Prediction | <a href="files/paper1.pdf" target="_blank">pdf</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Zhangzhi Peng (East China Jiaotong University, Nanchang, China)<br>Xiaohui Huang (East China Jiaotong University, Nanchang, China)
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                12:15
            </td>
            <td class="title-special">
                Lunch Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table class="plenary">
        <tr>
            <td class="date" rowspan="2">
                13:45
            </td>
            <td class="title">
                Learning and Reasoning with Conceptual Space Representations | <a href="files/keynote2.pdf" target="_blank">pdf</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Zied Bouraoui (Artois University, France)
            </td>
        </tr>
    </table>


    <table>
        <tr>
            <td class="date" rowspan="2">
                15:00
            </td>
            <td class="title">
                Learning Binary Classification Rules for Sequential Data | <a href="files/short1.pdf" target="_blank">pdf</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Marine Collery (IBM France Lab, Orsay, France + Inria Saclay Ile-de-France, Palaiseau, France)<br>Remy Kusters (IBM France Lab, Orsay, France + IBM Research, Orsay, France) 
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                15:20
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table class="plenary">
        <tr>
            <td class="date" rowspan="2">
                15:30
            </td>
            <td class="title">
                Neat and Scruffy: On Computational Generation and Interpretation of Spatial Descriptions | <a href="files/keynote3.pdf" target="_blank">pdf</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Simon Dobnik (University of Gothenburg, Sweden)
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                16:45
            </td>
            <td class="title">
                Scene Separation & Data Selection: Temporal Segmentation Algorithm for Real-time Video Stream Analysis | <a href="files/paper2.pdf" target="_blank">pdf</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Yuelin Xin (Southwest Jiaotong University, Chengdu, China + University of Leeds, United Kingdom)<br>Zihan Zhou (Southwest Jiaotong University, Chengdu, China)<br>Yuxuan Xia (Southwest Jiaotong University, Chengdu, China)
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                17:15
            </td>
            <td class="title">
                Challenges of Machine Learning Models Acting on Crystalline Materials | <a href="files/invited1.pdf" target="_blank">pdf</a>
            </td>
        </tr>
        <tr>
            <td class="speaker">
                Astrid Klipfel (Artois University, France)<br>Zied Bouraoui (Artois University, France)<br>Ya&euml;l Fr&eacute;gier (MIT Department of Mathematics, United States of America)<br>Adlane Sayede (Artois University, France)
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="1">
                17:35
            </td>
            <td class="title">
                Joint Discussion and Final Remarks
            </td>
        </tr>
    </table>
    <p><a href="#top">Go back to the top</a></p>

    <h2 id="Organization">Organization</h2>
    <h3>Organizing Committee</h3>
    <table class="sponsors">
        <tr>
            <td class="sponsor">
                <a target="_blank" href="https://aast.edu/cv.php?ser=118325/">
                    <img src="assets/Noha.jpg" alt="Noha Ghatwary" style="width: 50%; height: 50%">
                </a>
            </td>
            <td class="sponsor">
                <a target="_blank" href="https://zhiguolong.github.io/">
                    <img src="assets/Neda.jpeg" alt="Neda Azarmehr" style="width: 50%; height: 50%">
                </a>
            </td>
          
        </tr>
        <tr>
            <td>
                Dr. <a href="https://aast.edu/cv.php?ser=118325/" target="_blank">Noha Ghatwarys</a> is a Research Fellow with the Faculty of Information Systems and Applied Computer Sciences at the University of Bamberg, Germany. His general interests lie in Artificial Intelligence, Knowledge Representation and Reasoning, Data Mining, Logic Programming, and Semantic Web. His expertise lies in Qualitative Spatial and Temporal Reasoning.
            </td>
            <td>
                Dr. <a href="https://zhiguolong.github.io/" target="_blank">Neda Azarmehr</a> is a Lecturer with the School of Computing and Artificial Intelligence at the Southwest Jiaotong University, Chengdu, China. His research interests include fundamental and practical techniques in Knowledge Representation and Reasoning, especially in Qualitative Spatial and Temporal Reasoning, and representation problems in Machine Learning and Computer Vision.
            </td>
            
        </tr>
    </table> -->

    <h3>Program Committee</h3>
    <ul>
    <li><strong>Bettina Finzel</strong>, University of Bamberg, Germany</li>
    <li><strong>Bo Peng</strong>, Southwest Jiaotong University, Chengdu, China</li>

    
    </ul> 
    <p><a href="#top">Go back to the top</a></p>

    <h2 id="Venue">Venue</h2>
    <p>The workshop will take place on <strong>July 24, 2022</strong> in the room named <strong>Gallerie 11-12</strong> at the Messe Wien Exhibition and Congress Center, in Vienna, Austria.</p>

    <iframe class="directions" src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d10633.76857452723!2d16.407532!3d48.2173602!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x0%3A0x51b94dc6a5158516!2sMesse%20Wien%20Exhibition%20Congress%20Center!5e0!3m2!1sen!2sde!4v1647008253302!5m2!1sen!2sde" width="800" height="600" style="border:0;" allowfullscreen="" loading="lazy"></iframe>
    <p><a href="#top">Go back to the top</a></p>

    <footer>
        &copy; Special Session Organizers
        &nbsp;|&nbsp; Design by <a target="_blank" href="https://github.com/mikepierce">Mike Pierce</a>
        &nbsp;|&nbsp; Sponsored by <a target="_blank" href="https://ijcai-22.org/">IJCAI-ECAI-2022</a> and <a target="_blank" href="http://ceur-ws.org/">CEUR-WS.org</a>
    </footer>

</body>
</html>
